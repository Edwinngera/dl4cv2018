
from .dataset import Dataset
from .ops import Op
import torch
import numpy as np
import math

import typing

class Batch:
    '''
    A (mini)batch generated by the batch generator.
    '''

    def __init__(self):
        '''
        Ctor.
        '''

        self.data = None
        self.label = None
        self.idx = None

class BatchGenerator:
    '''
    Batch generator.
    Returned batches have the following properties:
      data: numpy array holding batch data of shape (s, SHAPE_OF_DATASET_SAMPLES).
      labels: numpy array holding batch labels of shape (s, SHAPE_OF_DATASET_LABELS).
      idx: numpy array with shape (s,) encoding the indices of each sample in the original dataset.
    '''

    def __init__(self, dataset: Dataset, num: int, shuffle: bool, op: Op = None):
        '''
        Ctor.
        Dataset is the dataset to iterate over.
        num is the number of samples per batch. the number in the last batch might be smaller than that.
        shuffle controls whether the sample order should be preserved or not.
        op is an operation to apply to input samples.
        Raises TypeError on invalid argument types.
        Raises ValueError on invalid argument values, such as if num is > len(dataset).
        '''

        # TODO implement

        #check Number of Samples per Batch
        if not isinstance(num, int) or num <= 0:
            raise ValueError("num should be a positive integeral value, "
                             "but got num={}".format(num))
        if num > len(dataset):
            raise ValueError("num should be smaller than the length of the dataset, "
                             "but got num={}".format(num))

        #check Dataset
        if not isinstance(dataset, Dataset):
            raise TypeError("dataset should be a Dataset type")
        if len(dataset)==0:
            raise ValueError("length of dataset is zero")

        self.dataset= dataset
        self.num = num
        self.shuffle = shuffle
        self.op = op


        if shuffle:
            self.iter = iter(torch.randperm (len (self.dataset)).tolist())
        else:
            self.iter = iter(range(len(self.dataset)))


    def __len__(self) -> int:
        '''
        Returns the number of batches generated per iteration.
        '''
        return math.ceil(len(self.dataset)/ self.num)



    def __iter__(self) -> typing.Iterable[Batch]:
        '''
        Iterate over the wrapped dataset, returning the data as batches.
        '''
        batch = Batch()
        data = []
        idx = []
        labels = []
        for sample_idx in self.iter:
            sample = self.dataset[sample_idx]
            if self.op:
                data.append(self.op(sample.data))
            else:
                data.append(sample.data)
            labels.append(sample.label)
            idx.append(sample.idx)
            if len(idx) == self.num:
                batch.data = np.asarray(data)
                batch.label = np.asarray(labels)
                batch.idx = np.asarray(idx)
                yield batch
                batch = Batch()
                data = []
                idx = []
                labels = []
        if len(idx) > 0:
            batch.data = np.asarray(data)
            batch.label = np.asarray(labels)
            batch.idx = np.asarray(idx)
            yield batch

